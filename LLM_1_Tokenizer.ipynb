{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8054aaf0",
   "metadata": {},
   "source": [
    "## LLM  Part 1 : Tokeniser \n",
    "\n",
    "Reference text : \n",
    "\n",
    "https://www.manning.com/books/build-a-large-language-model-from-scratch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112af175",
   "metadata": {},
   "source": [
    "### Problem Statement :\n",
    "\n",
    "The text we will tokenize for LLM training is a short story by Edith Wharton called The Verdict, which has been released into the public domain and is thus permitted to be used for LLM training tasks. The text is available on Wikisource at https://en.wikisource.org/wiki/The_Verdict,\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5e9846",
   "metadata": {},
   "source": [
    "### Step 1:  Read input file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "12a540d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word count is ->  20479\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "\n",
    "file1 = open(\"the-verdict.txt\", \"r+\", encoding=\"utf-8\")\n",
    "\n",
    "#print(\"Output of Read function is \")\n",
    "corpus = file1.read()\n",
    "#print(text)\n",
    "\n",
    "# check count of words \n",
    "print(\"word count is -> \", len(corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed7eead",
   "metadata": {},
   "source": [
    "### Step 2: Split text to tokens "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc7d5a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546e2347",
   "metadata": {},
   "source": [
    "####  Strategy 1 : Split on white spaces \n",
    "\n",
    "**Note**\n",
    "\n",
    "- The simple tokenization scheme below mostly works for separating the example text into individual words.\n",
    "- However, some words are still connected to punctuation characters that we want to have as separate list entries. \n",
    "- We also refrain from making all text lowercase because capitalization helps LLMs distinguish between proper nouns and common nouns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a266d89a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello,', ' ', 'world.', ' ', 'This,', ' ', 'is', ' ', 'a', ' ']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "text = \"Hello, world. This, is a test.\"\n",
    "result = re.split(r'(\\s)', text)\n",
    "print(result[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589e7400",
   "metadata": {},
   "source": [
    "#### Strategy 2 : Split on white space or comma or period.\n",
    "\n",
    "**Pattern Explanation**\n",
    "\n",
    "r'([,.]|\\s)': This is a raw string containing the regular expression pattern used to split the text.\n",
    "[,.]: Matches a comma , or a period ..\n",
    "|: This is the OR operator in regex, meaning that the pattern will match either the part before it or the part after it.\n",
    "\\s: Matches any whitespace character (spaces, tabs, newlines, etc.).\n",
    "() (parentheses): These are used to create a capturing group. Capturing groups save the matched text, so it appears in the result.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "469164b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', '', ' ', 'world', '.', '', ' ', 'This', ',']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "text = \"Hello, world. This, is a test.\"\n",
    "\n",
    "result = re.split(r'([,.]|\\s)', text)\n",
    "\n",
    "print(result[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9491f7",
   "metadata": {},
   "source": [
    "#### Strategy 3 : Split on white space or comma or period.\n",
    "\n",
    "The tokenization scheme we devised above works well on the simple sample text. Let's modify it a bit further so that it can also handle other types of punctuation, such as question marks, quotation marks, and the double-dashes we have seen earlier in the first 100 characters of Edith Wharton's short story, along with additional special characters:\n",
    "    \n",
    "\n",
    "**Pattern Explanation**\n",
    "\n",
    "[,.:;?_!\"()\\']: A character class that matches any single character inside the square brackets. This includes:\n",
    "\n",
    "\n",
    "- Comma\n",
    "- Period\n",
    "- Colon\n",
    "- Semicolon\n",
    "- Question mark \n",
    "- Underscore\n",
    "- Exclamation mark \n",
    "- Parentheses ()\n",
    "- Single quote \n",
    "- --: Matches the double hyphen --.\n",
    "- \\s: Matches any whitespace character (space, tab, newline, etc.).\n",
    "\n",
    "\n",
    "**Notes**\n",
    "The parentheses around the pattern create a capture group, meaning the matched delimiters are also included in the result.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c924277e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', '', ' ', 'world', '.', '', ' ', 'Is', ' ', 'this', '--', '', ' ', 'a', ' ', 'test', '?', '']\n"
     ]
    }
   ],
   "source": [
    "text = \"Hello, world. Is this-- a test?\"\n",
    "result = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1cd9017",
   "metadata": {},
   "source": [
    "### Step 3: Data cleaning - remove white space characters\n",
    "\n",
    "**Note  on white space character removal**\n",
    "\n",
    "- When developing a simple tokenizer, whether we should encode whitespaces as separate characters or just remove them depends on our application and its requirements. \n",
    "- Removing whitespaces reduces the memory and computing requirements. \n",
    "- However, keeping whitespaces can be useful if we train models that are sensitive to the exact structure of the text (for example, Python code, which is sensitive to indentation and spacing). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f432a913",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'world', '.', 'Is', 'this', '--', 'a', 'test', '?']\n"
     ]
    }
   ],
   "source": [
    "result = [item for item in result if item.strip()]\n",
    "print(result[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a50e2564",
   "metadata": {},
   "source": [
    "### Step 4 : Create. function \"text_to_tokens\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "31ef569e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import re\n",
    "\n",
    "def text_to_tokens(text: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Create an array of tokens from a given input text data. White spaces are removed.\n",
    "    Split takes care of special characters , which are treated as tokens also. \n",
    "\n",
    "    Parameters:\n",
    "    tokens (text: str ): A text string which needs to be tokenized \n",
    "\n",
    "    Returns:\n",
    "    List[str]: an list of tokens\n",
    "    \"\"\"\n",
    "    \n",
    "    # split text into tokens\n",
    "    result = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "    \n",
    "    # remove white spaces \n",
    "    result = [item for item in result if item.strip()]\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f5f33aea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31\n",
      "10\n",
      "['Hello', ',', 'world', '.', 'Is', 'this', '--', 'a', 'test', '?']\n"
     ]
    }
   ],
   "source": [
    "# check the function \n",
    "\n",
    "# function called \n",
    "tokenized = text_to_tokens(text)\n",
    "\n",
    "# check length of original text \n",
    "print(len(text))\n",
    "\n",
    "# check length after tokenization and removal of whitespace characters\n",
    "print(len(tokenized))\n",
    "\n",
    "# display 1st ten tokens \n",
    "print(tokenized[0:10])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a230f39e",
   "metadata": {},
   "source": [
    "### Step 5 : Build a Vocabulary.\n",
    "\n",
    "**Key Steps**\n",
    "\n",
    "- Take the tokenized text as input\n",
    "- Sort Alphabetically\n",
    "- Remove Duplicates \n",
    "- Create a Dictionary mapping individual tokens to a unique numeric ID \n",
    "\n",
    "### For this we will define a function \"create_vocab\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e973d793",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict\n",
    "\n",
    "def create_vocab(tokens: List[str], ) -> List [int]:\n",
    "    \"\"\"\n",
    "    Creates a Dictionary which maps a token to its token ID. The token inputs are sorted and duplicates a \n",
    "    removed before a dictionary is mapped \n",
    "\n",
    "    Parameters:\n",
    "    tokens (tokens: List[str]): A list of tokens\n",
    "\n",
    "    Returns:\n",
    "    Dict[str, int]: a vocabulary dictionary which maps a token to a unique tokenid.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # remove duplicates \n",
    "    unq_tokens = list(set(tokens))\n",
    "    \n",
    "    # sort \n",
    "    srt_tokens = sorted(unq_tokens)\n",
    "    \n",
    "    # create vocabulary\n",
    "    vocabulary = {token:tokenid for tokenid,token in enumerate(srt_tokens)}\n",
    "    \n",
    "    return vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8640edd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(',', 0)\n",
      "('--', 1)\n",
      "('.', 2)\n",
      "('?', 3)\n",
      "('Hello', 4)\n",
      "('Is', 5)\n",
      "('a', 6)\n",
      "('test', 7)\n",
      "('this', 8)\n",
      "('world', 9)\n"
     ]
    }
   ],
   "source": [
    "# call function\n",
    "vocab = create_vocab(tokenized)\n",
    "\n",
    "# Check - print 1st ten items of the vocabulary\n",
    "for i, item in enumerate(vocab.items()):\n",
    "    print(item)\n",
    "    if i > 10:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0125cfc5",
   "metadata": {},
   "source": [
    "### Step 6  : Create the Encoder \n",
    "\n",
    "**Key Steps**\n",
    "\n",
    "- Take any input text string and the pre defined vocabulary as input\n",
    "- Split the text to tokens \n",
    "- Use the vocabulary to generate tokenid for the input tokens \n",
    "- If a token does not exists in the vocabulary encode it with -99 \n",
    "\n",
    "### For this we will create a function  \"encode\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2f6536ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import List, Dict\n",
    "\n",
    "def encode(text: str, vocabulary: Dict[str, int]) -> List[int]:\n",
    "    \"\"\"\n",
    "    Encode the input text into a list of token IDs using the given vocabulary.\n",
    "\n",
    "    Parameters:\n",
    "    text (str): The input text string of tokens.\n",
    "    vocabulary (Dict[str, int]): A dictionary mapping tokens to integer values.\n",
    "\n",
    "    Returns:\n",
    "    List[int]: A list of integers representing the token IDs.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Split the input text into tokens\n",
    "    result = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "    \n",
    "    # remove white spaces \n",
    "    tokens = [item for item in result if item.strip()]\n",
    "    \n",
    "    \n",
    "    # Generate the list of token IDs using the vocabulary\n",
    "    token_ids = []\n",
    "    for token in tokens:\n",
    "        if token.strip() and token in vocabulary:\n",
    "            token_ids.append(vocabulary[token])\n",
    "        else:\n",
    "            # Handle unknown tokens if necessary (e.g., append a special token ID or skip)\n",
    "            # For example, let's append -1 for unknown tokens\n",
    "            token_ids.append(-99)\n",
    "    \n",
    "    return token_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "14c3e822",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 6, 7, -99]\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "vocabulary = {\n",
    "    \"Hello\": 1,\n",
    "    \"world\": 2,\n",
    "    \"!\": 3,\n",
    "    \"This\": 4,\n",
    "    \"is\": 5,\n",
    "    \"an\": 6,\n",
    "    \"example\": 7\n",
    "}\n",
    "\n",
    "text = \"Hello world! This is an example.\"\n",
    "encoded_text = encode(text, vocabulary)\n",
    "print(encoded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9196d482",
   "metadata": {},
   "source": [
    "### Step 7:  Create the Decoder \n",
    "\n",
    "**Notes**\n",
    "\n",
    "- When we want to convert the outputs of an LLM from numbers back into text, we also need a way to turn token IDs into text. \n",
    "\n",
    "- For this, we can create an inverse version of the vocabulary that maps token IDs back to corresponding text tokens.\n",
    "\n",
    "- **If an unknown token is passed for encoding - return a -99 for the same** \n",
    "\n",
    "#### We develop the \"decoder' function  as shown below "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "800ffe1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from typing import List, Dict\n",
    "\n",
    "def decode(vocabulary: Dict[str, int], token_ids: List[int]) -> List[str]:\n",
    "    \"\"\"\n",
    "    Decode the input list of token IDs into a list of string tokens using the given vocabulary.\n",
    "\n",
    "    Parameters:\n",
    "    vocabulary (Dict[str, int]): A dictionary mapping tokens to integer values.\n",
    "    token_ids (List[int]): A list of integers representing the token IDs.\n",
    "\n",
    "    Returns:\n",
    "    List[str]: A list of string tokens.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create a reverse dictionary from the vocabulary\n",
    "    int_to_str = {v: k for k, v in vocabulary.items()}\n",
    "    \n",
    "    # Generate the list of string tokens using the reverse dictionary\n",
    "    tokens = []\n",
    "    for token_id in token_ids:\n",
    "        if token_id in int_to_str:\n",
    "            tokens.append(int_to_str[token_id])\n",
    "        else:\n",
    "            # Handle unknown token IDs if necessary (e.g., append a special token or skip)\n",
    "            # For example, let's append -99 for unknown token IDs\n",
    "            tokens.append(-99)\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dfbfcaed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'world', '!', 'This', 'is', 'an', 'example']\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "\n",
    "# define a test vocab\n",
    "vocabulary = {\n",
    "    \"Hello\": 1,\n",
    "    \"world\": 2,\n",
    "    \"!\": 3,\n",
    "    \"This\": 4,\n",
    "    \"is\": 5,\n",
    "    \"an\": 6,\n",
    "    \"example\": 7\n",
    "}\n",
    "\n",
    "\n",
    "token_ids = [1, 2, 3, 4, 5, 6, 7]\n",
    "decoded_tokens = decode(vocabulary, token_ids)\n",
    "print(decoded_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8cf391b",
   "metadata": {},
   "source": [
    "### Step 8:  Build a final modified vocabulary function \n",
    "\n",
    "- Takes a list of raw strings \n",
    "- tokenizes them and removes white spaces \n",
    "- sorts the list \n",
    "- Adds a special token for unknown token \n",
    "- Adds a special token to mark end of text of a particular text source. \n",
    "- generates token id list as output \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0348020f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict\n",
    "\n",
    "def create_vocab(rawtext: List[str], ) -> List [int]:\n",
    "    \"\"\"\n",
    "    Creates a Dictionary which maps a token to its token ID. \n",
    "    Takes a list of raw strings \n",
    "    tokenizes them and removes white spaces \n",
    "    sorts the list \n",
    "    adds a special token for unknown token \n",
    "    adds a special token to mark end of text of a particular text source. \n",
    "    generates token id list as output \n",
    "\n",
    "\n",
    "    Parameters:\n",
    "    rawtext (rawtext: List[str]): A list of raw text strings\n",
    "\n",
    "    Returns:\n",
    "    Dict[str, int]: a vocabulary dictionary which maps a token to a unique tokenid.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # tokenize input text string  \n",
    "    tokens = re.split(r'([,.?_!\"()\\']|--|\\s)', rawtext)\n",
    "    \n",
    "    # remove white space \n",
    "    tokens = [item.strip() for item in tokens if item.strip()]\n",
    "    \n",
    "    # remove duplicates \n",
    "    unq_tokens = list(set(tokens))\n",
    "    \n",
    "    # sorted tokens\n",
    "    srt_tokens = sorted(unq_tokens)\n",
    "    \n",
    "    # add special tokens for unknown strings and end of text segment\n",
    "    srt_tokens.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
    "    \n",
    "    # create vocabulary\n",
    "    vocabulary = {token:tokenid for tokenid,token in enumerate(srt_tokens)}\n",
    "    \n",
    "    return vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "842ba607",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello world! This is an example.\n"
     ]
    }
   ],
   "source": [
    "# check text \n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "155c0543",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "{'!': 0, '.': 1, 'Hello': 2, 'This': 3, 'an': 4, 'example': 5, 'is': 6, 'world': 7, '<|endoftext|>': 8, '<|unk|>': 9}\n"
     ]
    }
   ],
   "source": [
    "# Create Vocabulary from  text \n",
    "vocab = create_vocab(text)\n",
    "\n",
    "# check length \n",
    "lenvocab = len(vocab)\n",
    "print(lenvocab)\n",
    "\n",
    "# print the vocab dict \n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c09e1b7",
   "metadata": {},
   "source": [
    "### Step 9 Build Vocabulary on short story by Edith Wharton called The Verdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "51dfba88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('!', 0), ('\"', 1), (\"'\", 2), ('(', 3), (')', 4)]\n",
      "[('younger', 1156), ('your', 1157), ('yourself', 1158), ('<|endoftext|>', 1159), ('<|unk|>', 1160)]\n"
     ]
    }
   ],
   "source": [
    "# create vocab \n",
    "vocab = create_vocab(corpus)\n",
    "\n",
    "# convert dict to list \n",
    "items_list = list(vocab.items())\n",
    "\n",
    "# Extract the first 5 items\n",
    "first_5_items = items_list[:5]\n",
    "\n",
    "# display and check \n",
    "print(first_5_items)\n",
    "\n",
    "# Extract the last 5 items\n",
    "last_5_items = items_list[-5:]\n",
    "\n",
    "# dispay and check \n",
    "print(last_5_items)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b522e3f2",
   "metadata": {},
   "source": [
    "###  Step 10 : Create the Tokenizer Class  \n",
    "(Code Reference - Ch 2 : Build a LLM from Scratch by Sebastian Raschka )\n",
    "\n",
    "- Here we implement a complete tokenizer class  with an encode method that splits text into tokens and carries out the string-to-integer mapping to produce token IDs via the vocabulary. \n",
    "\n",
    "- We add an <|unk|> token to represent new and unknown words that were not part of the training data and thus not part of the existing vocabulary. \n",
    "\n",
    "- Furthermore, we add an <|endoftext|> token that we can use to separate two unrelated text sources.\n",
    "\n",
    "- We also  implement a decode method that carries out the reverse integer-to-string mapping to convert the token IDs back into text.\n",
    "\n",
    "**Note on the decoder**\n",
    "\n",
    "We add an extra clean up step as follows \n",
    "\n",
    "- The code **re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)**\n",
    "\n",
    "removes any whitespace characters that appear immediately before specified punctuation marks, effectively tidying up the text by ensuring no spaces precede punctuation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "8a58df42",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenizerV1:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab \n",
    "        self.int_to_str = {tokenid:string for string,tokenid in vocab.items()} \n",
    "        \n",
    "        \n",
    "    def encode(self, text): \n",
    "        \n",
    "        # split input text into tokens\n",
    "        preprocessed = re.split(r'([,.?_!\"()\\']|--|\\s)', text)\n",
    "        \n",
    "        # remove white spaces \n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "         \n",
    "        # add special token  unknown                \n",
    "        preprocessed = [\n",
    "            item if item in self.str_to_int \n",
    "            else \"<|unk|>\" for item in preprocessed\n",
    "        ]\n",
    "\n",
    "        # Return list of token ids \n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "       \n",
    "                                                          \n",
    "    def decode(self, ids): \n",
    "        \n",
    "        # join the decoded tokens separated by one space \n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        \n",
    "        # removes any whitespace characters that appear immediately before specified punctuation marks, effectively tidying up the text by ensuring no spaces precede punctuation.\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text) \n",
    "        return text\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f9d3f5",
   "metadata": {},
   "source": [
    "## Test Tokenizer with basic text string "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "870f9f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define input text \n",
    "\n",
    "text1 = \"\"\" If no mistake have you made, yet losing you are,  a different game you should play. \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "eb0333b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate tokenizer with vocab \n",
    "tokenizer = TokenizerV1(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "a3f6ee45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[56, 725, 1160, 538, 1155, 669, 5, 1154, 1160, 1155, 174, 5, 119, 1160, 1160, 1155, 904, 1160, 7]\n"
     ]
    }
   ],
   "source": [
    "# encode and check \n",
    "ids = tokenizer.encode(text1)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "42551b56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If no <|unk|> have you made, yet <|unk|> you are, a <|unk|> <|unk|> you should <|unk|>.\n"
     ]
    }
   ],
   "source": [
    "# decode and check \n",
    "print(tokenizer.decode(ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e109949e",
   "metadata": {},
   "source": [
    "## Test Tokenizer with compound text string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "2255391a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, do you wish to have coffee? <|endoftext|> In the shade of the large palm trees\n"
     ]
    }
   ],
   "source": [
    "# define inout text \n",
    "text1 = \"Hello, do you wish to have coffee?\"\n",
    "\n",
    "text2 = \"In the shade of the large palm trees\"\n",
    "\n",
    "text = \" <|endoftext|> \".join((text1, text2))\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "863026e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1160, 5, 362, 1155, 1135, 1042, 538, 1160, 10, 1159, 57, 1013, 898, 738, 1013, 1160, 1160, 1160]\n"
     ]
    }
   ],
   "source": [
    "# Instantiate tokenizer with vocab \n",
    "tokenizer = TokenizerV1(vocab)\n",
    "\n",
    "print(tokenizer.encode(text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "ac849fb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|unk|>, do you wish to have <|unk|>? <|endoftext|> In the shade of the <|unk|> <|unk|> <|unk|>\n"
     ]
    }
   ],
   "source": [
    "# decode\n",
    "print(tokenizer.decode(tokenizer.encode(text)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7438aca0",
   "metadata": {},
   "source": [
    "## End of notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c3f9f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
