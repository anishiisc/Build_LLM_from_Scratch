{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8054aaf0",
   "metadata": {},
   "source": [
    "## LLM  Part 2 :  Byte Pair Encoding\n",
    "\n",
    "**Reference text** \n",
    "\n",
    "https://www.manning.com/books/build-a-large-language-model-from-scratch\n",
    "\n",
    "**Text Corpus** \n",
    "\n",
    "The text we will tokenize for LLM training is a short story by Edith Wharton called The Verdict, which has been released into the public domain and is thus permitted to be used for LLM training tasks. The text is available on Wikisource at https://en.wikisource.org/wiki/The_Verdict,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112af175",
   "metadata": {},
   "source": [
    "### Problem Statement :\n",
    "\n",
    "In the first notebook, we discussed how to develop a Word Tokenizer step by step. In this notebook we will demonstrate An advanced tokenization method called Byte Pair Encoding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1c2fa3",
   "metadata": {},
   "source": [
    "### Additional special tokens that could have been implemented further in the tokenizer developed in Part 1 \n",
    "\n",
    "- [BOS] (beginning of sequence): This token marks the start of a text. It signifies to the LLM where a piece of content begins.\n",
    "\n",
    "- [EOS] (end of sequence): This token is positioned at the end of a text, and is especially useful when concatenating multiple unrelated texts, similar to <|endoftext|>. For instance, when combining two different Wikipedia articles or books, the [EOS] token indicates where one article ends and the next one begins.\n",
    "\n",
    "- [PAD] (padding): When training LLMs with batch sizes larger than one,\n",
    "the batch might contain texts of varying lengths. To ensure all texts have the same length, the shorter texts are extended or \"padded\" using the [PAD] token, up to the length of the longest text in the batch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b93dda",
   "metadata": {},
   "source": [
    "### What does the tokenizer for GPT models use ?\n",
    "\n",
    "- The tokenizer used for GPT models does not need any of these tokens mentioned above but only uses an <|endoftext|> token for simplicity. \n",
    "\n",
    "- The <|endoftext|> is analogous to the [EOS] token mentioned above. Also, <|endoftext|> is used for padding as well. However, as we'll explore in subsequent chapters when training on batched inputs, we typically use a mask, meaning we don't attend to padded tokens. Thus, the specific token chosen for padding becomes inconsequential.\n",
    "\n",
    "- Moreover, the tokenizer used for GPT models also doesn't use an <|unk|> token for out-of-vocabulary words.\n",
    "\n",
    "- Instead, GPT models use a **byte pair encoding tokenizer**, which breaks down words into subword units, which we will discuss in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e63639",
   "metadata": {},
   "source": [
    "### How is the Byte Pair Encoding used by GPT-2 superior ?\n",
    "\n",
    "\n",
    "- it allows the model to break down words that aren't in its predefined vocabulary into smaller subword units or even individual characters, enabling it to handle out-of-vocabulary words\n",
    "- For instance, if GPT-2's vocabulary doesn't have the word \"unfamiliarword,\" it might tokenize it as [\"unfam\", \"iliar\", \"word\"] or some other subword breakdown, depending on its trained BPE merges\n",
    "- The original BPE tokenizer can be found here: [https://github.com/openai/gpt-2/blob/master/src/encoder.py](https://github.com/openai/gpt-2/blob/master/src/encoder.py)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00fb133",
   "metadata": {},
   "source": [
    "## Concept Note : Byte Pair Encoding \n",
    "\n",
    "REFERENCE \n",
    "https://www.geeksforgeeks.org/byte-pair-encoding-bpe-in-nlp/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed7eead",
   "metadata": {},
   "source": [
    "#### Concepts related to BPE:\n",
    "\n",
    "- **Vocabulary:** A set of subword units that can be used to represent a text corpus.\n",
    "- **Byte:** A unit of digital information that typically consists of eight bits.\n",
    "- **Character:** A symbol that represents a written or printed letter or numeral.\n",
    "- **Frequency:** The number of times a byte or character occurs in a text corpus.\n",
    "- **Merge:** The process of combining two consecutive bytes or characters to create a new subword unit.\n",
    "\n",
    "\n",
    "### Steps involved in BPE:\n",
    "- Initialize the vocabulary with all the bytes or characters in the text corpus\n",
    "- Calculate the frequency of each byte or character in the text corpus.\n",
    "- Repeat the following steps until the desired vocabulary size is reached:\n",
    "    - Find the most frequent pair of consecutive bytes or characters in the text corpus\n",
    "    - Merge the pair to create a new subword unit.\n",
    "    - Update the frequency counts of all the bytes or characters that contain the merged pair.\n",
    "    -  Add the new subword unit to the vocabulary.\n",
    "- Represent the text corpus using the subword units in the vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c45af8",
   "metadata": {},
   "source": [
    "### How does BPE Work - A simple example \n",
    "\n",
    "Suppose we have a text corpus with the following four words: “ab”, “bc”, “bcd”, and “cde”. The initial vocabulary consists of all the bytes or characters in the text corpus: {“a”, “b”, “c”, “d”, “e”}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546e2347",
   "metadata": {},
   "source": [
    "####  Step 1 : Initialize the vocabulary \n",
    "\n",
    "Vocabulary = {\"a\", \"b\", \"c\", \"d\", \"e\"}\n",
    "\n",
    "####  Step 2 : Compute Frequency  \n",
    "\n",
    "Frequency = {\"a\": 1, \"b\": 3, \"c\": 3, \"d\": 2, \"e\": 1}\n",
    "\n",
    "#### Repeat Steps 3 to 5 until the desired vocabulary size is reached.\n",
    "\n",
    "- ####  Step 3 : Find the most frequent pair of two characters\n",
    "\n",
    "        The most frequent pair is \"bc\" with a frequency of 2.\n",
    "\n",
    "- ####  Step 4 :  Merge the pair\n",
    "\n",
    "        Merge \"bc\" to create a new subword unit \"bc\"\n",
    "\n",
    "- #### Step 5: Update frequency counts\n",
    "\n",
    "        Frequency = {\"a\": 1, \"b\": 2, \"c\": 3, \"d\": 2, \"e\": 1, \"bc\": 2}\n",
    "        \n",
    "#### Represent the text corpus using subword units\n",
    "\n",
    "The resulting vocabulary consists of the following subword units: {\"a\", \"b\", \"c\", \"d\", \"e\", \"bc\", \"cd\", \"de\",\"ab\",\"bcd\",\"cde\"}."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e92d6c69",
   "metadata": {},
   "source": [
    "## Section A:  Python Implementation of Byte Pair Encoding \n",
    "\n",
    "We will define a series of functions to perfome the byte pair encoding as discussed above\n",
    "\n",
    "\n",
    "#### 1) get_vocab()\n",
    "\n",
    "The get_vocab function is defined to take a list of strings (data) as input and return a dictionary mapping words (formatted as separated characters with an end token) to their frequency counts.\n",
    "\n",
    "\n",
    "#### 2) get_stats()\n",
    "\n",
    "The get_stats function is defined to take a dictionary (vocab) as input and return a dictionary mapping tuples of character pairs to their frequency counts.\n",
    "    \n",
    "#### 3) merge_vocab()\n",
    "\n",
    "The merge_vocab function is defined to take a tuple of characters (pair) and a dictionary (v_in) as input, and return a new dictionary with the specified pair of characters merged.\n",
    "\n",
    "\n",
    "byte pai \n",
    "\n",
    "The byte_pair_encoding function is defined to take a list of strings (data) and an integer (n) as input, and return a dictionary representing the vocabulary with merged character pairs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589e7400",
   "metadata": {},
   "source": [
    "### STEP 1.  Define function get_vocab()\n",
    "\n",
    "\n",
    "#### Note on creating the vocabulary dictionary**\n",
    "\n",
    "- **vocab = defaultdict(int):** Initializes a defaultdict with int as the default factory, meaning any new key will have a default value of 0.\n",
    "\n",
    "- The function iterates through each line in data and then through each word in the line.\n",
    "\n",
    "- **vocab[' '.join(list(word)) + ' </w>'] += 1:**\n",
    "\n",
    "- **list(word):** Converts the word into a list of characters.\n",
    "\n",
    "- **' '.join(list(word)): Joins the characters with spaces.**\n",
    "\n",
    "- **+ ' </w>': Adds an end token </w> to the end of the word.**\n",
    "\n",
    "- The resulting string is used as a key in the vocab dictionary, and its value (frequency count) is incremented by 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "469164b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from typing import List, Dict\n",
    "\n",
    "def get_vocab(data: List[str]) -> Dict[str, int]:\n",
    "    \"\"\"\n",
    "    Given a list of strings, returns a dictionary of words mapping to their frequency \n",
    "    count in the data.\n",
    "\n",
    "    Parameters:\n",
    "    data (List[str]): A list of strings where each string is a line of text.\n",
    "\n",
    "    Returns:\n",
    "    Dict[str, int]: A dictionary where keys are words with separated characters and\n",
    "                    an end token, and values are their frequency counts.\n",
    "    \"\"\"\n",
    "    vocab = defaultdict(int)\n",
    "    for line in data:\n",
    "        for word in line.split():\n",
    "            # Join the characters of the word with spaces and add an end token\n",
    "            vocab[' '.join(list(word)) + ' </w>'] += 1\n",
    "    return vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57a06733",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>, {'t h i s </w>': 3, 'i s </w>': 3, 'a </w>': 3, 't e s t </w>': 4, 'o n l y </w>': 1})\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "data = [\n",
    "    \"this is a test\",\n",
    "    \"this test is only a test\",\n",
    "    \"a test this is\"\n",
    "]\n",
    "\n",
    "vocab = get_vocab(data)\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62fdc019",
   "metadata": {},
   "source": [
    "### STEP 2:   Define function get_stats()\n",
    "\n",
    "\n",
    "\n",
    "#### Note on Creating the Pairs Dictionary:\n",
    "\n",
    "- **pairs = defaultdict(int):** Initializes a defaultdict with int as the default factory, meaning any new key will have a default value of 0.\n",
    "\n",
    "- The function iterates through each word and its frequency in vocab.\n",
    "\n",
    "- **symbols = word.split():** Splits the word into its component symbols (characters and end token).\n",
    "\n",
    "\n",
    "- The nested loop iterates through adjacent symbol pairs in the list and increments their frequency count in the pairs dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "754158be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from typing import Dict, Tuple\n",
    "\n",
    "def get_stats(vocab: Dict[str, int]) -> Dict[Tuple[str, str], int]:\n",
    "    \"\"\"\n",
    "    Given a vocabulary (dictionary mapping words to frequency counts), returns a \n",
    "    dictionary of tuples representing the frequency count of pairs of characters \n",
    "    in the vocabulary.\n",
    "\n",
    "    Parameters:\n",
    "    vocab (Dict[str, int]): A dictionary where keys are words with separated characters \n",
    "                            and an end token, and values are their frequency counts.\n",
    "\n",
    "    Returns:\n",
    "    Dict[Tuple[str, str], int]: A dictionary where keys are tuples of character pairs \n",
    "                                and values are their frequency counts in the vocabulary.\n",
    "    \"\"\"\n",
    "    pairs = defaultdict(int)\n",
    "    for word, freq in vocab.items():\n",
    "        symbols = word.split()\n",
    "        for i in range(len(symbols) - 1):\n",
    "            pairs[symbols[i], symbols[i + 1]] += freq\n",
    "    return pairs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3764c9d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>, {('t', 'h'): 3, ('h', 'i'): 3, ('i', 's'): 5, ('s', '</w>'): 5, ('a', '</w>'): 2, ('t', 'e'): 2, ('e', 's'): 2, ('s', 't'): 2, ('t', '</w>'): 2})\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "vocab = {\n",
    "    't h i s </w>': 3,\n",
    "    'i s </w>': 2,\n",
    "    'a </w>': 2,\n",
    "    't e s t </w>': 2\n",
    "}\n",
    "\n",
    "stats = get_stats(vocab)\n",
    "print(stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11ef8c6",
   "metadata": {},
   "source": [
    "### STEP 3:   Define function merge_vocab()\n",
    "\n",
    "\n",
    "**Notes on Creating the New Vocabulary Dictionary**\n",
    "\n",
    "\n",
    "- v_out = {}: Initializes an empty dictionary for the new vocabulary.\n",
    "\n",
    "- bigram = re.escape(' '.join(pair)): Joins the pair with a space and escapes any special characters for use in a regular expression.\n",
    "\n",
    "- p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)'): Compiles a regular expression pattern that matches the bigram as a whole word (using negative lookbehind (?<!\\S) and negative lookahead (?!\\S) to ensure it is not part of another word).\n",
    "\n",
    "- The function iterates through each word in v_in.\n",
    "  -   w_out = p.sub(''.join(pair), word): Replaces occurrences of the bigram in the word with the merged pair.\n",
    "  -   v_out[w_out] = v_in[word]: Adds the modified word and its frequency to the new vocabulary dictionary.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e46249",
   "metadata": {},
   "source": [
    "### STEP 3a).    Creating Bigrams \n",
    "\n",
    "**Detailed explanation of:**\n",
    "\n",
    "```python\n",
    "bigram = re.escape(' '.join(pair)):\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af39baf8",
   "metadata": {},
   "source": [
    "**Objective:** This line constructs a regular expression pattern for a bigram (pair of characters) and escapes any special characters.\n",
    "\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "- pair is assumed to be a tuple containing two strings, representing the bigram.\n",
    "- ' '.join(pair) combines the two strings with a space in between.\n",
    "- re.escape() escapes any special characters in the resulting string to ensure they are treated as literal characters in the regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab2de95b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\\ b\n"
     ]
    }
   ],
   "source": [
    "### Example \n",
    "\n",
    "import re\n",
    "pair = ('a', 'b')\n",
    "bigram = re.escape(' '.join(pair))\n",
    "# ' '.join(pair) results in 'a b'\n",
    "# re.escape('a b') results in 'a\\\\ b' (the space is escaped)\n",
    "print(bigram)  # Output: 'a\\\\ b'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d671399",
   "metadata": {},
   "source": [
    "###  STEP 3b) Compiling  the Regex pattern \n",
    "\n",
    "**Detailed explanation of:**\n",
    "\n",
    "```python\n",
    " p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)'):\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e03f74",
   "metadata": {},
   "source": [
    "**Objective:** This line creates a compiled regular expression object p that matches the bigram when it appears as a whole word, not part of another word.\n",
    "\n",
    "\n",
    "**Components:**\n",
    "\n",
    "- r'(?<!\\S)': A negative lookbehind assertion. \\S matches any non-whitespace character. (?<!\\S) asserts that what immediately precedes the current position is not a non-whitespace character (i.e., it ensures the bigram is not part of a larger word).\n",
    "\n",
    "- bigram: The escaped bigram pattern from the previous step.\n",
    "\n",
    "\n",
    "- r'(?!\\S)': A negative lookahead assertion. \\S matches any non-whitespace character. (?!\\S) asserts that what immediately follows the current position is not a non-whitespace character (i.e., it ensures the bigram is not part of a larger word).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7fc74caf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a b', 'a b', 'a b']\n"
     ]
    }
   ],
   "source": [
    "# Example \n",
    "\n",
    "import re\n",
    "pair = ('a', 'b')\n",
    "\n",
    "bigram = re.escape(' '.join(pair))\n",
    "\n",
    "p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n",
    "\n",
    "text = \"a b a b a b a bc\"\n",
    "matches = p.findall(text)\n",
    "# Matches 'a b' only when it appears as a whole word, not part of 'a bc'\n",
    "print(matches)  # Output: ['a b', 'a b', 'a b']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c10526",
   "metadata": {},
   "source": [
    "### STEP 3c Create output Dict\n",
    "\n",
    "**Detailed Explanation of below code snippet** \n",
    "\n",
    "\n",
    "```python\n",
    "for word in v_in:\n",
    "    w_out = p.sub(''.join(pair), word)\n",
    "    v_out[w_out] = v_in[word]\n",
    "\n",
    "```\n",
    "\n",
    "**Objective:** The code snippet merges the character pair if the pair is find in any word string\n",
    "\n",
    "**Steps:**\n",
    "- Loop through the words\n",
    "- for each word search for the character pair\n",
    "- If a match is found substute the space separated pair by the joined pair (no spaces) \n",
    "  add an new entry to the output dictionary (new key) with value equal to the frequency of the old dictionary    before the substitution \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0155e11d",
   "metadata": {},
   "source": [
    "### STEP  3d)  Let us code the merge_vocab function now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "434526f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import Tuple, Dict\n",
    "\n",
    "def merge_vocab(pair: Tuple[str, str], v_in: Dict[str, int]) -> Dict[str, int]:\n",
    "    \"\"\"\n",
    "    Given a pair of characters and a vocabulary, returns a new vocabulary with the \n",
    "    pair of characters merged together wherever they appear.\n",
    "\n",
    "    Parameters:\n",
    "    pair (Tuple[str, str]): A tuple containing two characters to be merged.\n",
    "    v_in (Dict[str, int]): A dictionary where keys are words with separated characters \n",
    "                           and an end token, and values are their frequency counts.\n",
    "\n",
    "    Returns:\n",
    "    Dict[str, int]: A new vocabulary dictionary with the pair of characters merged.\n",
    "    \"\"\"\n",
    "    v_out = {}\n",
    "    bigram = re.escape(' '.join(pair))\n",
    "    p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n",
    "    \n",
    "    \n",
    "    \n",
    "    for word in v_in:\n",
    "        w_out = p.sub(''.join(pair), word)\n",
    "        v_out[w_out] = v_in[word]\n",
    "    return v_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "62deed58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'th i s </w>': 3, 'i s </w>': 2, 'a </w>': 2, 't e s t </w>': 2}\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "v_in = {\n",
    "    't h i s </w>': 3,\n",
    "    'i s </w>': 2,\n",
    "    'a </w>': 2,\n",
    "    't e s t </w>': 2\n",
    "}\n",
    "\n",
    "pair = ('t', 'h')\n",
    "merged_vocab = merge_vocab(pair, v_in)\n",
    "print(merged_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041d079f",
   "metadata": {},
   "source": [
    "### STEP 4 : Create the Byte Pair Encoder Function \n",
    "\n",
    "\n",
    "**Putting it all together**\n",
    "\n",
    "\n",
    "**Input Parameters**\n",
    "\n",
    "- A list of strings \n",
    "- an integer n denoting how many merged pairs are to be returned \n",
    "\n",
    "**Output Parameter(s)**\n",
    "\n",
    "- A dictionary with the vocabulary post Byte Pair Encoding \n",
    "\n",
    "\n",
    "\n",
    "**Key Process Steps**\n",
    "\n",
    "- vocab = get_vocab(data): Initializes the vocabulary using the get_vocab function.\n",
    "- loop through 'n' times, at each iteration:\n",
    "    - determine frequency dict of  character pairs\n",
    "    - extract the most frequent character pair\n",
    "    - merge the most frequent pair in the vocab list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b9917e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict\n",
    "\n",
    "def byte_pair_encoder(data: List[str], n: int) -> Dict[str, int]:\n",
    "    \"\"\"\n",
    "    Given a list of strings and an integer n, returns a list of n merged pairs\n",
    "    of characters found in the vocabulary of the input data.\n",
    "\n",
    "    Parameters:\n",
    "    data (List[str]): A list of strings where each string is a line of text.\n",
    "    n (int): The number of pairs of characters to merge.\n",
    "\n",
    "    Returns:\n",
    "    Dict[str, int]: A dictionary representing the vocabulary with merged character pairs.\n",
    "    \"\"\"\n",
    "    vocab = get_vocab(data)\n",
    "    for i in range(n):\n",
    "        pairs = get_stats(vocab)\n",
    "        best = max(pairs, key=pairs.get)\n",
    "        vocab = merge_vocab(best, vocab)\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d7959892",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "\n",
    "# Set corpus \n",
    "\n",
    "corpus = '''Tokenization is the process of breaking down \n",
    "a sequence of text into smaller units called tokens,\n",
    "which can be words, phrases, or even individual characters.\n",
    "Tokenization is often the first step in natural language processing tasks \n",
    "such as text classification, named entity recognition, and sentiment analysis.\n",
    "The resulting tokens are typically used as input to further processing steps,\n",
    "such as vectorization, where the tokens are converted\n",
    "into numerical representations for machine learning models to use.'''\\\n",
    "\n",
    "\n",
    "# split by sentence \n",
    "data = corpus.split('.')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c186f956",
   "metadata": {},
   "source": [
    "#### TEST BPE with n = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "047a0ffc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Tokenization</w>': 2, 'is</w>': 2, 'the</w>': 3, 'process</w>': 1, 'of</w>': 2, 'breaking</w>': 1, 'down</w>': 1, 'a</w>': 1, 'sequence</w>': 1, 'text</w>': 2, 'into</w>': 2, 'smaller</w>': 1, 'units</w>': 1, 'called</w>': 1, 'tokens,</w>': 1, 'which</w>': 1, 'can</w>': 1, 'be</w>': 1, 'words,</w>': 1, 'phrases,</w>': 1, 'or</w>': 1, 'even</w>': 1, 'individual</w>': 1, 'characters</w>': 1, 'often</w>': 1, 'first</w>': 1, 'step</w>': 1, 'in</w>': 1, 'natural</w>': 1, 'language</w>': 1, 'processing</w>': 2, 'tasks</w>': 1, 'such</w>': 2, 'as</w>': 3, 'classification,</w>': 1, 'named</w>': 1, 'entity</w>': 1, 'recognition,</w>': 1, 'and</w>': 1, 'sentiment</w>': 1, 'analysis</w>': 1, 'The</w>': 1, 'resulting</w>': 1, 'tokens</w>': 2, 'are</w>': 2, 'typically</w>': 1, 'used</w>': 1, 'input</w>': 1, 'to</w>': 2, 'further</w>': 1, 'steps,</w>': 1, 'vectorization,</w>': 1, 'where</w>': 1, 'conv er te d</w>': 1, 'n u m er ic al</w>': 1, 're pr es en t ation s</w>': 1, 'f or</w>': 1, 'm a ch in e</w>': 1, 'l e ar n ing</w>': 1, 'm o d e l s</w>': 1, 'us e</w>': 1}\n"
     ]
    }
   ],
   "source": [
    "# define output count \n",
    "n = 200\n",
    "\n",
    "# call function \n",
    "bpe_pairs = byte_pair_encoder(data, n)\n",
    "\n",
    "# check \n",
    "print(bpe_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf09d17",
   "metadata": {},
   "source": [
    "#### TEST BPE with n = 210"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dbd11b0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Tokenization</w>': 2, 'is</w>': 2, 'the</w>': 3, 'process</w>': 1, 'of</w>': 2, 'breaking</w>': 1, 'down</w>': 1, 'a</w>': 1, 'sequence</w>': 1, 'text</w>': 2, 'into</w>': 2, 'smaller</w>': 1, 'units</w>': 1, 'called</w>': 1, 'tokens,</w>': 1, 'which</w>': 1, 'can</w>': 1, 'be</w>': 1, 'words,</w>': 1, 'phrases,</w>': 1, 'or</w>': 1, 'even</w>': 1, 'individual</w>': 1, 'characters</w>': 1, 'often</w>': 1, 'first</w>': 1, 'step</w>': 1, 'in</w>': 1, 'natural</w>': 1, 'language</w>': 1, 'processing</w>': 2, 'tasks</w>': 1, 'such</w>': 2, 'as</w>': 3, 'classification,</w>': 1, 'named</w>': 1, 'entity</w>': 1, 'recognition,</w>': 1, 'and</w>': 1, 'sentiment</w>': 1, 'analysis</w>': 1, 'The</w>': 1, 'resulting</w>': 1, 'tokens</w>': 2, 'are</w>': 2, 'typically</w>': 1, 'used</w>': 1, 'input</w>': 1, 'to</w>': 2, 'further</w>': 1, 'steps,</w>': 1, 'vectorization,</w>': 1, 'where</w>': 1, 'converted</w>': 1, 'numerical</w>': 1, 'repres en t ation s</w>': 1, 'f or</w>': 1, 'm a ch in e</w>': 1, 'l e ar n ing</w>': 1, 'm o d e l s</w>': 1, 'us e</w>': 1}\n"
     ]
    }
   ],
   "source": [
    "# define output count \n",
    "n = 210\n",
    "\n",
    "# call function \n",
    "bpe_pairs = byte_pair_encoder(data, n)\n",
    "\n",
    "# check \n",
    "print(bpe_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf8d6c3",
   "metadata": {},
   "source": [
    "## Section B: Using Byte Pair Encoder from tiktoken \n",
    "\n",
    "**Note**\n",
    "\n",
    "We have seen how complex it is to implement BPE from ground up! and its also operationally expensive in compute sense.\n",
    "\n",
    "In this section we  will use an existing Python open-source library called tiktoken (https://github.com/openai/tiktoken), which implements the BPE algorithm very efficiently based on source code in **Rust**.\n",
    "\n",
    "\n",
    "### installs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8d3036db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tiktoken\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f7c1d9",
   "metadata": {},
   "source": [
    "### Instantiate the BPE tokenizer from tiktoken "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b1294148",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267ccb06",
   "metadata": {},
   "source": [
    "### Check usage \n",
    "\n",
    "**Key Steps**\n",
    " - tokenize an input text to token ids and check \n",
    " - convert token ids back to tokens and check \n",
    "\n",
    "\n",
    "#### encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a13e1297",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15496, 11, 466, 345, 765, 617, 6891, 30, 220, 50256, 554, 262, 16187, 286, 1588, 18057, 7150, 1659, 617, 34680, 27271, 13]\n"
     ]
    }
   ],
   "source": [
    "# define text \n",
    "text = (\n",
    "    \"Hello, do you want some coffee? <|endoftext|> In the shadows of large palm trees\"\n",
    "     \"of someunknownPlace.\"\n",
    ")\n",
    "\n",
    "# tokenize \n",
    "integers = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "# check \n",
    "print(integers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1e5c63",
   "metadata": {},
   "source": [
    "#### decode "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c924277e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, do you want some coffee? <|endoftext|> In the shadows of large palm treesof someunknownPlace.\n"
     ]
    }
   ],
   "source": [
    "strings = tokenizer.decode(integers)\n",
    "\n",
    "print(strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca755ab7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a1cd9017",
   "metadata": {},
   "source": [
    "### Observations from usage \n",
    "\n",
    "-  **First** The <|endoftext|> token is assigned a relatively large token ID, namely, 50256. In fact, the BPE tokenizer, which was used to train models such as GPT-2, GPT-3, and the original model used in ChatGPT, has a total vocabulary size of 50,257, with <|endoftext|> being assigned the largest token ID.\n",
    "\n",
    "\n",
    "- **Second** The BPE tokenizer above encodes and decodes unknown words, such as \"someunknownPlace\" correctly. The BPE tokenizer can handle any unknown word. **How does it achieve this without using <|unk|> tokens?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b68d41",
   "metadata": {},
   "source": [
    "#### The Trick is as below\n",
    "\n",
    "The algorithm underlying BPE breaks down words that aren't in its\n",
    "predefined vocabulary into smaller subword units or even individual characters, enabling it to handle out-of-vocabulary words. So, thanks to the BPE algorithm, if the tokenizer encounters an unfamiliar word during tokenization, it can represent it as a sequence of subword tokens or characters, as illustrated in Figure below.\n",
    "\n",
    "#### Fig reference - Ch2 Reference text \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962bbacb",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/11.webp\" width=\"500px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc836bc",
   "metadata": {},
   "source": [
    "### Let us test the above "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d680ad40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9084, 86, 343, 86, 220, 959]\n",
      " Akwirw ier\n"
     ]
    }
   ],
   "source": [
    "text = \" Akwirw ier\"\n",
    "\n",
    "# tokenize \n",
    "integers = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "# check \n",
    "print(integers)\n",
    "\n",
    "\n",
    "strings = tokenizer.decode(integers)\n",
    "\n",
    "print(strings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a50e2564",
   "metadata": {},
   "source": [
    "### Food for thought: - How does it recombine unknown words post byte pair encoding!!!\n",
    "\n",
    "(guess we need to ask that to some one in open AI!!!)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7438aca0",
   "metadata": {},
   "source": [
    "## End of notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c3f9f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
